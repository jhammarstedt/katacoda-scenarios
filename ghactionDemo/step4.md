# Running the Pytest benchmarking tool


<img src="https://github.com/jhammarstedt/katacoda-scenarios/blob/main/ghactionDemo/images/tut_part2.PNG?raw=true" height="400" />


To run the pytest benchmarking tool, we need a Python script to benchmark and another one to format the output from pytest. We create a Python script by running the following commands in the terminal:

 
`touch benchmarking.py`{{execute}}

`touch data/generate_output.py`{{execute}}
   
We then insert the following code into 'benchmarking.py' using a text editor:

``` 
import pytest
import time


def turtle(n=1000000):
    """Regular for loop"""
    x = [] 
    for i in range(n,0,-1):
        x.append(str(i))
    
    s= ""
    for i in x:
        s+= i


def cheetah(n=1000000):
    """Faster with list comprehenssion and join"""
    x = [str(i) for i in range(n,0,-1)]
    s = "".join(x)
    
def test_case(benchmark):
    benchmark(turtle)
``` 

This Python script contains several functions that can be used for the performance test. We then need to insert the following code into the 'data/generate_output.py' script:


```   
import json
from bs4 import BeautifulSoup
import os


"""
This file will read the latest output file generated by the workflow and take out relevant statistics.
As this tutorial is more to show of the workflow and how to set up a github action than building websites,
we'll be simply inserting the new data in the index.html file instead of having a js file that would read the json file.
which requires some more integration.
"""

with open("output.json","r") as f: 
    data = json.load(f)

# First we get the new data statistics as a markdown to the index file
commit = data["commit_info"]["id"]
date = data["commit_info"]["time"]
mean = data["benchmarks"][0]["stats"]["mean"]
mini = data["benchmarks"][0]["stats"]["mean"]
maxi = data["benchmarks"][0]["stats"]["max"]
std = data["benchmarks"][0]["stats"]["stddev"]

# The format of the index.html is to place new entires in <tr> = <tablerow> which will be inside a tableBody
new_data = f"""<tr>
                <td>{commit} </td>
                <td>{date}</td>
                <td>{mean}</td>
                <td>{maxi}</td>
                <td>{mini}</td>
                <td>{std}</td>
            </tr>"""

# Using bs4 we can now just insert the new data to the table, maybe not best practice but it works :)
html = open('docs/index.html')
soup = BeautifulSoup(html,'html.parser')
soup.tbody.tr.append(BeautifulSoup( new_data,'html.parser'))
html.close()

new_html = soup.prettify(soup.original_encoding)
with open("docs/index.html",'w') as f:
      f.write(new_html)
      f.close()

```

The 'data/generate_output.py' script formats the json file we get when running pytest. It presents the results in an html file using tables. 

We have to add the benchmarking and formatting scripts to our GitHub action. Open the '.github/workflows/python.yml' file using a text editor and insert the following code below the last line:

```   
                pytest benchmarking.py --benchmark-json output.json
                python data/generate_output.py

```   

This code will run the pytest benchmarking tool on the 'benchmarking.py' script and the results will be stored in the 'output.json' file. The 'data/generate_output.py' will then format the results stored in 'output.json' and store it in the 'index.html' file. 



                                
Your '.github/workflows/python.yml' file should now look like this:

```   
name: Python benchmarking using pytest
on: push
jobs:
        benchmark:
                name: pytest-benchmarking
                runs-on: ubuntu-latest
                steps:
                        - uses: actions/checkout@v2
                          with:
                                persist-credentials: false
                                fetch-depth: 0 
                        - uses: actions/setup-python@v1
                        - name: Installing and running pytest
                          run: |
                                pwd
                                python -m pip install --upgrade pip
                                if [ -f requirements.txt ]; 
                                then pip install -r requirements.txt; fi
                                python test.py
                                pytest benchmarking.py --benchmark-json output.json
                                python data/generate_output.py
``` 

