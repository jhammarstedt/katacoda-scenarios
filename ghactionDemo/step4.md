# Running the Pytest benchmarking tool


<img src="https://github.com/jhammarstedt/katacoda-scenarios/blob/main/ghactionDemo/images/tut_part2.PNG?raw=true" height="400" />


To run the pytest benchmarking tool, we need a Python script to benchmark and another one to format the output from pytest. We start by creating the benchmarking script with the functions we wish to test:
 
`touch src/benchmarking.py`{{execute}}

We then insert the following code into 'benchmarking.py' using a text editor:

```{{copy}}
import pytest

# Here are some example functions to run, can of course be replaced by your own to test things out
def turtle(n=1000000):
    """Regular for loops"""
    x = [] 
    for i in range(n,0,-1):
        x.append(str(i))
    
    s= ""
    for i in x:
        s+= i


def cheetah(n=1000000):
    """Faster with list comprehenssion and join"""
    x = [str(i) for i in range(n,0,-1)]
    s = "".join(x)

# This is where pytest gets called   
def test_case(benchmark):
    benchmark(turtle)
```

This Python script contains several functions that can be used for the performance test. The important addition is to actually pass the `benchmark` fixture to the test_case function. When then calling `benchmark()` pytest will run `benchmark(test_function)` on any function passed to it. For more info go [here](https://pypi.org/project/pytest-benchmark/)

Now we have our 


`touch src/generate_output.py`{{execute}}

We then need to insert the following code into the 'src/generate_output.py' script:


```{{copy}}
import json
from bs4 import BeautifulSoup
import os


"""
This file will read the latest output file generated by the workflow and take out relevant statistics.
As this tutorial is more to show of the workflow and how to set up a github action than building websites,
we'll be simply inserting the new data in the index.html file instead of having a js file that would read the json file.
which requires some more integration.
"""

with open("output.json","r") as f: 
    data = json.load(f)

# First we get the new data statistics as a markdown to the index file
commit = data["commit_info"]["id"]
date = data["commit_info"]["time"]
mean = data["benchmarks"][0]["stats"]["mean"]
mini = data["benchmarks"][0]["stats"]["mean"]
maxi = data["benchmarks"][0]["stats"]["max"]
std = data["benchmarks"][0]["stats"]["stddev"]

# The format of the index.html is to place new entires in <tr> = <tablerow> which will be inside a tableBody
new_data = f"""<tr>
                <td>{commit} </td>
                <td>{date}</td>
                <td>{mean}</td>
                <td>{maxi}</td>
                <td>{mini}</td>
                <td>{std}</td>
            </tr>"""

# Using bs4 we can now just insert the new data to the table, maybe not best practice but it works :)
html = open('docs/index.html')
soup = BeautifulSoup(html,'html.parser')
soup.tbody.tr.append(BeautifulSoup( new_data,'html.parser'))
html.close()

new_html = soup.prettify(soup.original_encoding)
with open("docs/index.html",'w') as f:
      f.write(new_html)
      f.close()

```

The 'data/generate_output.py' script formats the json file we get when running pytest. It presents the results in an html file using tables. 

We have to add the benchmarking and formatting scripts to our GitHub action. Open the '.github/workflows/python.yml' file using a text editor and insert the following code below the last line:

```{{copy}} 
                pytest benchmarking.py --benchmark-json output.json
                python data/generate_output.py

```   

This code will run the pytest benchmarking tool on the 'benchmarking.py' script and the results will be stored in the 'output.json' file. The 'data/generate_output.py' will then format the results stored in 'output.json' and store it in the 'index.html' file. 



                                
Your '.github/workflows/python.yml' file should now look like this:

```{{copy}}   
name: Python benchmarking using pytest
on: push
jobs:
        benchmark:
                name: pytest-benchmarking
                runs-on: ubuntu-latest
                steps:
                        - uses: actions/checkout@v2
                          with:
                                persist-credentials: false
                                fetch-depth: 0 
                        - uses: actions/setup-python@v1
                        - name: Installing and running pytest
                          run: |
                                pwd
                                python -m pip install --upgrade pip
                                if [ -f requirements.txt ]; 
                                then pip install -r requirements.txt; fi
                                python test.py
                                pytest benchmarking.py --benchmark-json output.json
                                python data/generate_output.py
```

